{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38cbe71f",
   "metadata": {},
   "source": [
    "## NLP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181e5fc6",
   "metadata": {},
   "source": [
    "### 1 Data Collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9a842f",
   "metadata": {},
   "source": [
    "### 2 Data Processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a33d4d",
   "metadata": {},
   "source": [
    "#### 2.1 Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60760a8",
   "metadata": {},
   "source": [
    "##### 2.1.1 Remove Special Characters\n",
    "\n",
    "Use regular expressions or string replacement operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1053e27e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, World!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, <b>World</b>!\"\n",
    "cleaned_text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48551cc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is an example sentence.\n"
     ]
    }
   ],
   "source": [
    "text = \"This   is  an example sentence.\\n\\n\\n\"\n",
    "cleaned_text = ' '.join(text.split())\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a23ee35",
   "metadata": {},
   "source": [
    "##### 2.1.2 Convert to Lowercase\n",
    "\n",
    "Standardize the text's case.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab969bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello World\"\n",
    "lowercase_text = text.lower()\n",
    "print(lowercase_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc473adb",
   "metadata": {},
   "source": [
    "##### 2.1.3 Spelling Correction\n",
    "\n",
    "Use spelling correction tools (e.g., NLTK, TextBlob).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dbcf078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"Helllo Worlld\"\n",
    "corrected_text = TextBlob(text).correct()\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e0d881",
   "metadata": {},
   "source": [
    "#### 2.2 Tokenization\n",
    "\n",
    "Break down a text into individual words or tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4211bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'an', 'example', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is an example sentence.\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711a5d4",
   "metadata": {},
   "source": [
    "#### 2.3 Stopwords Removal\n",
    "\n",
    "Use a list of stopwords to remove common but low-information words, such as \"and,\" \"the,\" etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e00ba5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example sentence .\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    words = word_tokenize(text)\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "text = \"This is an example sentence.\"\n",
    "filtered_text = remove_stopwords(text)\n",
    "print(filtered_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e932c3a",
   "metadata": {},
   "source": [
    "#### 2.4 Stemming / Lemmatization\n",
    "\n",
    "Reduces words to their base or dictionary form.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd1d5be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word = \"running\"\n",
    "lemma = lemmatizer.lemmatize(word, pos=\"v\")\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1515920",
   "metadata": {},
   "source": [
    "### 3 Text Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5627af02",
   "metadata": {},
   "source": [
    "#### 3.1 Syntax Analysis\n",
    "\n",
    "Analyze sentence structure, including identifying words, phrases, and clauses within the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9737bad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This nsubj is\n",
      "is ROOT is\n",
      "an det sentence\n",
      "example compound sentence\n",
      "sentence attr is\n",
      ". punct is\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"This is an example sentence.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395c1d32",
   "metadata": {},
   "source": [
    "#### 3.2 NER\n",
    "\n",
    "Identify and extract named entities from the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "718364fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Inc. ORG\n",
      "Steve Jobs PERSON\n",
      "Cupertino GPE\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "text = \"Apple Inc. was founded by Steve Jobs in Cupertino.\"\n",
    "doc = nlp(text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b129fc",
   "metadata": {},
   "source": [
    "#### 3.3 Sentiment Analysis\n",
    "\n",
    "Analyze the sentiment of the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "847150bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive sentiment\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "text = \"I love this markdown.\"\n",
    "blob = TextBlob(text)\n",
    "sentiment = blob.sentiment.polarity\n",
    "if sentiment > 0:\n",
    "    print(\"Positive sentiment\")\n",
    "elif sentiment < 0:\n",
    "    print(\"Negative sentiment\")\n",
    "else:\n",
    "    print(\"Neutral sentiment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491b2d0b",
   "metadata": {},
   "source": [
    "#### 3.4 Topic Modeling\n",
    "\n",
    "Apply algorithms such as LDA or NMF to discover topics within the text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53744d3",
   "metadata": {},
   "source": [
    "### 4 Feature Extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671324c8",
   "metadata": {},
   "source": [
    "#### 4.1 Bag of Words\n",
    "\n",
    "BoW represents text data as a vector, where each element of the vector represents the frequency of a corresponding word in the text. This model assumes that the words in the text are independent of each other and ignores the order and grammatical structure of the words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b86a10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Vector:\n",
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 2 0 1 0 1 1 0 1]\n",
      " [1 0 0 1 1 0 1 1 1]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "Vocabulary:\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create a text dataset\n",
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?'\n",
    "]\n",
    "\n",
    "# Create the Bag of Words model\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the text dataset into a Bag of Words vector\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the vocabulary\n",
    "vocab = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the Bag of Words vector to a sparse matrix\n",
    "print(\"Bag of Words Vector:\")\n",
    "print(X.toarray())\n",
    "\n",
    "# Display the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126549c3",
   "metadata": {},
   "source": [
    "**Ignoring Word Order and Grammar**\n",
    "\n",
    "The BoW model treats words in text as independent, thus disregarding word order and grammatical structure. This means it cannot capture contextual information between words, leading to semantic loss.\n",
    "\n",
    "**High-Dimensional Sparsity**\n",
    "\n",
    "BoW-generated vectors are often highly sparse, especially when the vocabulary is large or the text dataset is extensive. This results in a high-dimensional feature space, which may require more storage and computational resources.\n",
    "\n",
    "**Inability to Handle New Vocabulary**\n",
    "\n",
    "When constructing the vocabulary in BoW, it assumes that all words in the text dataset are known. It struggles to effectively handle unknown vocabulary when new words appear.\n",
    "\n",
    "**Not Suitable for Long Texts**\n",
    "\n",
    "For long texts, BoW features may become very large, potentially leading to the curse of dimensionality, particularly in classification tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51e41a5",
   "metadata": {},
   "source": [
    "#### 4.2 TF-IDF\n",
    "\n",
    "TF-IDF, which stands for Term Frequency-Inverse Document Frequency, is widely used in text feature extraction aimed at assessing the importance of words within a document or a corpus of documents. \n",
    "\n",
    "**Term Frequency - TF:**\n",
    "\n",
    "- TF represents how frequently a word occurs within a document.\n",
    "- Typically, it can be calculated as the number of times a word appears in the document (raw term frequency) or as a normalized version, such as relative term frequency (the term frequency divided by the total number of words in the document).\n",
    "- TF measures a word's importance within an individual document.\n",
    "\n",
    "**Inverse Document Frequency - IDF:**\n",
    "\n",
    "- IDF assesses a word's importance within the entire corpus of documents.\n",
    "\n",
    "- Words with a higher IDF value are considered unique to the entire corpus because they do not appear frequently in all documents.\n",
    "\n",
    "- The IDF is often calculated using the formula: \n",
    "  $$\n",
    "  IDF(t)=log(\\frac{N + 1}{1 + df(t)}) + 1\n",
    "  $$\n",
    "  where $N$ is the total number of documents, and $df(t)$ is the number of documents that contain the term $t$.\n",
    "\n",
    "**TF-IDF Calculation:**\n",
    "\n",
    "- The TF-IDF value is the product of TF and IDF.\n",
    "- It measures a word's importance within a document while taking into account the importance of the word within the entire corpus.\n",
    "- Words with high TF-IDF values are those that occur frequently within an individual document but are rare in the entire dataset, indicating their uniqueness.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9630116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF Vectors:\n",
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n",
      "Vocabulary:\n",
      "['and' 'document' 'first' 'is' 'one' 'second' 'the' 'third' 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create a text dataset\n",
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This document is the second document.\",\n",
    "    \"And this is the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "\n",
    "# Create a TF-IDF model\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Transform the text dataset into TF-IDF vectors\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Get the vocabulary\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF vectors into a sparse matrix\n",
    "print(\"TF-IDF Vectors:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "# Display the vocabulary\n",
    "print(\"Vocabulary:\")\n",
    "print(feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf41a78",
   "metadata": {},
   "source": [
    "#### 4.3 N-gram\n",
    "\n",
    "N-gram is used to split text into consecutive n words or character sequences. The main idea of N-gram is to capture local features and contextual information in text.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f18cc6",
   "metadata": {},
   "source": [
    "#### 4.4 Word Embeddings\n",
    "\n",
    "Word Embeddings is a technique for mapping words into a continuous vector space, where each word is represented as a vector. These vectors are automatically learned from large-scale text data, often using deep learning methods such as Word2Vec, GloVe, or FastText.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6867ee48",
   "metadata": {},
   "source": [
    "### 5 Machine Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9431ef9",
   "metadata": {},
   "source": [
    "### 6 Training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611ab24c",
   "metadata": {},
   "source": [
    "#### 6.1 Overfitting\n",
    "\n",
    "##### 6.1.1 L1 Regularization - Lasso\n",
    "\n",
    "Add a penalty term to the loss function that is the sum of the absolute values of the model's weights. It encourages the model to have sparse weight values, effectively performing feature selection by pushing some weights to exactly zero. The term is often calculated using the formula:\n",
    "$$\n",
    "l1 = \\lambda\\sum_{i=1}^n|w_i|\n",
    "$$\n",
    "where $\\lambda$ is the regularization strength hyperparameter, and $w_i$ represents the model's weight for the $i^{th}$ feature.\n",
    "\n",
    "##### 6.1.2 L2 Regularization - Ridge\n",
    "\n",
    "Add a penalty term to the loss function that is the sum of the squares of the model's weights. It encourages the model to have smaller and more evenly distributed weights, preventing any single feature from dominating. The term is often calculated using the formula:\n",
    "$$\n",
    "l2 = \\lambda\\sum_{i=1}^n w_i^2\n",
    "$$\n",
    "\n",
    "##### 6.1.3 Dropout\n",
    "\n",
    "Select a subset of neurons and set their outputs to zero during each training iteration according to a parameter $p$, which controls the probability of deactivating each neuron. This method effectively enforce the network to train with different subnetworks for each training sample, reducing their dependence on the model. This helps increase the generalization ability of the model, making the network more robust and generalizable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef36d88",
   "metadata": {},
   "source": [
    "### 7 Evaluation\n",
    "\n",
    "- *TP* = True Positives (correctly classified positive instances)\n",
    "- *TN* = True Negatives (correctly classified negative instances)\n",
    "- *FP* = False Positives (incorrectly classified as positive)\n",
    "- *FN* = False Negatives (incorrectly classified as negative)\n",
    "\n",
    "#### 7.1 Accuracy\n",
    "\n",
    "Represent the ratio of correctly classified samples to the total number of samples.\n",
    "$$\n",
    "Accuracy = \\frac{TP+TN}{TP+TN+FP+FN}\n",
    "$$\n",
    "\n",
    "\n",
    "#### 7.2 Precision\n",
    "\n",
    "Show the model's ability to correctly classify positive cases among all cases it classified as positive.\n",
    "$$\n",
    "Precision = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "\n",
    "#### 7.3 Recall\n",
    "\n",
    "Show the model's ability to correctly classify positive cases among all actual positive cases. \n",
    "$$\n",
    "Recall = \\frac{TP}{TP+FN}\n",
    "$$\n",
    "\n",
    "\n",
    "#### 7.4 F1 Score\n",
    "\n",
    "The harmonic mean of precision and recall.\n",
    "$$\n",
    "F1 = \\frac{2 \\cdot Precision \\cdot Recall}{Precision+Recall}\n",
    "$$\n",
    "\n",
    "\n",
    "#### 7.5 ROC Curve / AUC\n",
    "\n",
    "It plots the False Positive Rate (FPR) on the x-axis and the True Positive Rate (TPR) on the y-axis for different threshold values. The area under the ROC curve (AUC) is also a metric used to measure a classification model's performance. A higher AUC value (closer to 1) indicates better model performance.\n",
    "$$\n",
    "FPR= \\frac{FP}{FP+TN}\n",
    "$$\n",
    "$$\n",
    "TPR=Recall=\\frac{TP}{TP+FN}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247bb4b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
